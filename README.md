# Bipedal Walker ğŸš€

This project presents a comparative study of three major reinforcement learning algorithms:

* **PPO** (Proximal Policy Optimization) ğŸŸ¢
* **SAC** (Soft Actor-Critic) ğŸ”µ
* **TD3** (Twin Delayed Deep Deterministic Policy Gradient) ğŸŸ 

All agents are trained on the continuous control environment:

**BipedalWalker-v3 (Gymnasium)** ğŸ¦¾

The objective is to analyze training stability, convergence behavior, and overall performance across different RL paradigms (on-policy vs off-policy).

---

## Project Structure ğŸ“‚

```
Bipedal_Walker/
â”‚
â”œâ”€â”€ bipedal_walker.ipynb
â”œâ”€â”€ Bipedal_Walker.pdf
â”œâ”€â”€ bipedalwalker_algorithms_comparison.png
â”œâ”€â”€ bipedalwalker_ppo.gif
â”œâ”€â”€ bipedalwalker_sac.gif
â”œâ”€â”€ bipedalwalker_td3.gif
â”‚
â”œâ”€â”€ ppo_logs/
â”œâ”€â”€ sac_logs/
â””â”€â”€ td3_logs/
```

---

## Environment ğŸŒ³

* **Environment:** BipedalWalker-v3
* **Library:** Gymnasium
* **Framework:** Stable-Baselines3
* **Training timesteps per algorithm:** 100,000

This is a continuous control task where the agent must learn to walk using a 4-joint robot.

---

## Algorithms Overview ğŸ§ 

### PPO ğŸŸ¢

* On-policy actor-critic method
* Uses clipped objective to stabilize updates
* Parallel environments (8) used for faster experience collection

### SAC ğŸ”µ

* Off-policy method
* Maximizes entropy for better exploration
* Automatic entropy tuning

### TD3 ğŸŸ 

* Off-policy deterministic algorithm
* Twin critics to reduce overestimation bias
* Delayed policy updates
* Target policy smoothing
* Gaussian action noise

---

## Evaluation Metrics ğŸ“Š

Training performance is analyzed using:

* Episode reward (raw)
* Episode reward (rolling mean over 50 episodes)
* Episode length (raw)
* Episode length (rolling mean over 50 episodes)

The rolling mean smooths noisy reward signals and reveals convergence trends more clearly.

---

## Results ğŸ¯

The generated comparison plot includes:

1. Raw episode rewards
2. Smoothed episode rewards
3. Raw episode lengths
4. Smoothed episode lengths

The visualization highlights differences in:

* Stability
* Convergence speed
* Sample efficiency

Additionally, GIFs of the trained agents are generated to qualitatively compare learned behaviors.

---

## Key Observations âœ¨

* **PPO** learns relatively fast but may exhibit instability.
* **SAC** provides smoother and more stable learning.
* **TD3** improves more gradually but remains stable.

This comparison illustrates trade-offs between exploration strategy, stability, and policy update mechanisms.
Complete details can be found on `Bipedal_Walker.pdf`.

---

## How to Run ğŸƒâ€â™‚ï¸

Install dependencies:

```bash
pip install gymnasium stable-baselines3 matplotlib pandas imageio
```

Then run the notebook:

```bash
jupyter notebook bipedal_walker.ipynb
```

---

## Outputs Generated ğŸ–¼ï¸

* `bipedalwalker_algorithms_comparison.png`
* `bipedalwalker_ppo.gif`
* `bipedalwalker_sac.gif`
* `bipedalwalker_td3.gif`

---

## ğŸ“œ License

This project is released under the [MIT](https://github.com/sepanta007/Bipedal_Walker/blob/master/LICENSE) License. 